{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmrpEmYEw0s9"
      },
      "source": [
        "Based on https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxYGkP7e0VJW"
      },
      "source": [
        "Env preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uShrd17R1uFG",
        "outputId": "c14b7283-57af-46d1-9722-375c72b6899d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdmGoLIK203S",
        "outputId": "5e914a89-58c0-4a4f-8136-f84f854df903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MLMI\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/MLMI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX2aEnWXfzHN",
        "outputId": "6700ad9f-d708-4adb-c68e-3657f0941b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting medmnist\n",
            "  Downloading medmnist-2.1.0-py3-none-any.whl (21 kB)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from medmnist) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.11.0+cu113)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from medmnist) (0.18.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.3.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from medmnist) (7.1.2)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from medmnist) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->medmnist) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->medmnist) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->medmnist) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (1.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (1.4.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->medmnist) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->medmnist) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->medmnist) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (2022.6.15)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=524f811185ef78fb6cfc40c152edac6fce4d062c55574332485f153f8a1a1b83\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, torchmetrics, medmnist\n",
            "Successfully installed fire-0.4.0 medmnist-2.1.0 torchmetrics-0.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install medmnist torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeVIe7LwfzHR",
        "outputId": "e93f324f-3ed5-41c5-bc51-abc767a1d3fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f36ba7eadb0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import random\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch import logsumexp, zeros_like, ones_like\n",
        "\n",
        "import torchvision\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import utils, models\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(10)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i00JiH9CfzHT",
        "outputId": "656ed762-8a0c-4b9d-e2d5-1212cf22f435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedMNIST v2.1.0 @ https://github.com/MedMNIST/MedMNIST/\n"
          ]
        }
      ],
      "source": [
        "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6UVRt_NfzHU"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nr2QAz31fzHV",
        "outputId": "c2a0a651-fd25-4799-b24c-202f82813019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multi-class'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_flag = 'pathmnist'\n",
        "# data_flag = 'breastmnist'\n",
        "download = True\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "PATIENCE = 30\n",
        "lr = 0.002\n",
        "weight_decay = 0.001\n",
        "number_of_res_blocks = 5\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labeled_subset(dataset, n_classes=9, per_class = 1000):\n",
        "  label_buckets = [[] for _ in range(0, n_classes)]\n",
        "  for img, label in dataset:\n",
        "    label_buckets[label.item()].append((img, label))\n",
        "\n",
        "  label_sub_bucket = [[] for _ in range(0, n_classes)]\n",
        "  \n",
        "  for i, bucket in enumerate(label_buckets):\n",
        "    label_sub_bucket[i] = random.sample(bucket, per_class)\n",
        "\n",
        "  balanced_labelled = []\n",
        "  for b in label_sub_bucket:\n",
        "    balanced_labelled.extend(b)\n",
        "\n",
        "  return data.DataLoader(balanced_labelled, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Yob6aQi66cOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obn5ng6xfzHX"
      },
      "source": [
        "MedMNIST data, preprocessing, data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM7T8VrOfzHX",
        "outputId": "fce46024-717f-4721-a539-9c0c2ce2bf0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n"
          ]
        }
      ],
      "source": [
        "# preprocessing\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "val_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader= data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "unlabeled_subset = Subset(train_dataset, range(int(len(train_dataset) * 0.5)))\n",
        "unlabeled_loader = data.DataLoader(dataset=unlabeled_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "labeled_loader = create_labeled_subset(train_dataset, per_class=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "eL9fqJIYfzHb",
        "outputId": "5eac3584-80d7-403c-bfa9-a4bb0c4a8ec0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F0133F05550>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# visualization\n",
        "\n",
        "train_dataset.montage(length=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRPW1PRlfzHb"
      },
      "outputs": [],
      "source": [
        "# montage\n",
        "m = train_dataset.montage(length=10)\n",
        "m.save('train.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Kh7_ZAfzHc"
      },
      "source": [
        "ResNet model definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJRuhcYbtSYW"
      },
      "outputs": [],
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0-mB3wttTel"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()      \n",
        "        self.info = 'in ' +  str(in_channels) + ', out ' + str(out_channels) + ', stride ' + str(stride)\n",
        "        if stride > 1 or in_channels != out_channels:\n",
        "            # Add strides in the skip connection and zeros for the new channels.\n",
        "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride], (0, 0, 0, 0, 0, out_channels - in_channels), mode=\"constant\", value=0))\n",
        "        else:\n",
        "            self.skip = nn.Sequential()\n",
        " \n",
        "        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        bn1 = nn.BatchNorm2d(out_channels)\n",
        "        conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same')\n",
        "        bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.l1 = nn.Sequential(conv1, bn1)\n",
        "        self.l2 = nn.Sequential(conv2, bn2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        skip = self.skip(input)\n",
        "        x = self.l1(input)\n",
        "        x = F.relu(x)\n",
        "        x = self.l2(x)\n",
        "        return F.relu(x + skip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9H6YzF8tV4T"
      },
      "outputs": [],
      "source": [
        "class ResidualStack(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
        "        super().__init__()\n",
        "        first = [ResidualBlock(in_channels, out_channels, stride)]\n",
        "        rest = [ResidualBlock(out_channels, out_channels) for i in range(num_blocks - 1)]\n",
        "        self.modules_list = nn.Sequential(*(first + rest))\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return self.modules_list(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK1hx3GZfzHd"
      },
      "source": [
        "Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeZ7Q-ddtc3o"
      },
      "outputs": [],
      "source": [
        "def initialize_weight(module):\n",
        "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "    elif isinstance(module, nn.BatchNorm2d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xf9BeHWtiOQ",
        "outputId": "9cb243a6-9424-4e21-d7c6-903a61c4f3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeVupiWbfzHe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def run_epoch(model, optimizer, dataloader, criterion):\n",
        "  \n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "\n",
        "    for inputs, targets in tqdm(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        if task == 'multi-label, binary-class':\n",
        "            targets = targets.to(torch.float32)\n",
        "        else:\n",
        "            targets = targets.squeeze().long()\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "            \n",
        "        top1 = torch.argmax(outputs, dim=1).long()\n",
        "        ncorrect = torch.sum(top1 == targets)\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += ncorrect.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss /= len(dataloader.dataset)\n",
        "    epoch_acc /= len(dataloader.dataset)\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekwq5bJtAQAX"
      },
      "outputs": [],
      "source": [
        "best_model_weights = None\n",
        "\n",
        "def fit(model, optimizer, lr_scheduler, task):\n",
        "\n",
        "    if task == \"multi-label, binary-class\":\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "    else:\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    _ = model.to(device)\n",
        "    _ = model.apply(initialize_weight)\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    curr_patience = 0.0\n",
        "    min_val_loss = 1.0\n",
        "\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "      train_loss, train_acc = run_epoch(model, optimizer, labeled_loader, criterion)\n",
        "      lr_scheduler.step()\n",
        "      print(f\"Epoch {epoch + 1: >3}/{NUM_EPOCHS}, train loss: {train_loss:.2e}, train acc: {train_acc:.3f}\")\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      train_accuracies.append(train_acc)\n",
        "      \n",
        "      val_loss, val_acc, val_top5 = test(model, val_loader, 'val', criterion)\n",
        "      print(f\"Epoch {epoch + 1: >3}/{NUM_EPOCHS}, val loss: {val_loss:.2e}, val acc: {val_acc:.3f}, val top5: {val_top5:.3f}\")\n",
        "\n",
        "      val_losses.append(val_loss)\n",
        "      val_accuracies.append(val_acc)\n",
        "\n",
        "      if val_loss >= min_val_loss:\n",
        "        curr_patience += 1\n",
        "        if curr_patience == PATIENCE:\n",
        "          break\n",
        "      else:\n",
        "        curr_patience = 0\n",
        "        min_val_loss = val_loss\n",
        "        best_model_weights = copy.deepcopy(model.state_dict())\n",
        "        torch.save(model.state_dict(), 'ResNet.pt')\n",
        "\n",
        "    model.load_state_dict(best_model_weights)\n",
        "\n",
        "    plt.plot(range(len(val_losses)), np.array(val_losses))\n",
        "    plt.title('val loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(range(len(val_accuracies)), np.array(val_accuracies))\n",
        "    plt.title('val acc')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(range(len(train_losses)), np.array(train_losses))\n",
        "    plt.title('train loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(range(len(train_accuracies)), np.array(train_accuracies))\n",
        "    plt.title('train acc')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VCEKl-QfzHf"
      },
      "outputs": [],
      "source": [
        "def test(model, data_loader, mode, criterion):\n",
        "    model.eval()\n",
        "    epoch_acc = 0.0\n",
        "    epoch_acc_top5 = 0.0\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if task == 'multi-label, binary-class':\n",
        "                targets = targets.to(torch.float32)\n",
        "            else:\n",
        "                targets = targets.squeeze().long()\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            top1 = torch.argmax(outputs, dim=1).long()\n",
        "            ncorrect = torch.sum(top1 == targets)\n",
        "\n",
        "            _ , top5 = torch.topk(outputs, 5, dim=1)\n",
        "            top5 = top5.t()\n",
        "            correct = top5.eq(targets.reshape(1, -1).expand_as(top5))\n",
        "            ncorrect_top5 = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += ncorrect.item()\n",
        "            epoch_acc_top5 += ncorrect_top5.item()\n",
        "\n",
        "        \n",
        "        epoch_loss /= len(data_loader.dataset)\n",
        "        epoch_acc /= len(data_loader.dataset)  \n",
        "        epoch_acc_top5 /= len(data_loader.dataset)  \n",
        "    \n",
        "    return epoch_loss, epoch_acc, epoch_acc_top5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca5a9O83cdZM",
        "outputId": "49961ecd-0eed-4964-a041-ac1796c018ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "if task == 'multi-label, binary-class':\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeJqfFWVoLbA"
      },
      "outputs": [],
      "source": [
        "res_net_18 = nn.Sequential(\n",
        "          nn.Conv2d(n_channels, 64, kernel_size=7, stride=1, padding='same', bias=False),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(),\n",
        "          ResidualStack(64, 64, 1, 6),\n",
        "          ResidualStack(64, 128, 2, 8),\n",
        "          ResidualStack(128, 256, 2, 12),\n",
        "          ResidualStack(256, 512, 2, 6),\n",
        "          nn.AdaptiveAvgPool2d(1),\n",
        "          Lambda(lambda x: x.squeeze()),\n",
        "          nn.Linear(512, n_classes)\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW1s8KfQwyTF"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(res_net_18.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n",
        "\n",
        "# fit(res_net_18, optimizer, lr_scheduler, task) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeajiLRAxFN4"
      },
      "outputs": [],
      "source": [
        "# test(res_net_18, test_loader, 'test', criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1TU07cPxpx_"
      },
      "outputs": [],
      "source": [
        "# test(res_net_18, val_loader, 'val', criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH707hWBf20V"
      },
      "outputs": [],
      "source": [
        "def test(model, data_loader, mode, criterion):\n",
        "    model.eval()\n",
        "    epoch_acc = 0.0\n",
        "    epoch_acc_top5 = 0.0\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            _, outputs = model(inputs)\n",
        "\n",
        "            if task == 'multi-label, binary-class':\n",
        "                targets = targets.to(torch.float32)\n",
        "            else:\n",
        "                targets = targets.squeeze().long()\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            top1 = torch.argmax(outputs, dim=1).long()\n",
        "            ncorrect = torch.sum(top1 == targets)\n",
        "\n",
        "            _ , top5 = torch.topk(outputs, 5, dim=1)\n",
        "            top5 = top5.t()\n",
        "            correct = top5.eq(targets.reshape(1, -1).expand_as(top5))\n",
        "            ncorrect_top5 = correct[:5].reshape(-1).float().sum(0, keepdim=True)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += ncorrect.item()\n",
        "            epoch_acc_top5 += ncorrect_top5.item()\n",
        "\n",
        "        \n",
        "        epoch_loss /= len(data_loader.dataset)\n",
        "        epoch_acc /= len(data_loader.dataset)  \n",
        "        epoch_acc_top5 /= len(data_loader.dataset)  \n",
        "    \n",
        "    return epoch_loss, epoch_acc, epoch_acc_top5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXeNzrScypLy"
      },
      "source": [
        "SGAN\n",
        "based on https://github.com/opetrova/SemiSupervisedPytorchGAN/blob/master/SemiSupervisedGAN.ipynb and https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/sgan/sgan.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vmj-btNZyqHv",
        "outputId": "6fc18f7b-042f-48b5-842b-6660b2e9db2d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0/100, train loss: 1.89e-02, train acc: 7.379972443219699\n",
            "Epoch   0/100, val loss: 1.19e-01, val acc: 0.19812075169932028, val top5: 0.6930227908836465\n",
            "(0.07211635321959811, 0.31963788300835655, 0.8558495821727019)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1/100, train loss: 1.21e-02, train acc: 7.508733721498733\n",
            "Epoch   1/100, val loss: 2.59e-02, val acc: 0.5615753698520591, val top5: 0.9513194722111156\n",
            "(0.024981832786522867, 0.6207520891364903, 0.9681058495821727)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   2/100, train loss: 9.74e-03, train acc: 7.571958753722388\n",
            "Epoch   2/100, val loss: 1.94e-02, val acc: 0.6204518192722911, val top5: 0.9737105157936825\n",
            "(0.030812517979018867, 0.5966573816155989, 0.9692200557103064)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   3/100, train loss: 8.72e-03, train acc: 7.6251833414818435\n",
            "Epoch   3/100, val loss: 1.73e-02, val acc: 0.6629348260695722, val top5: 0.9854058376649341\n",
            "(0.014252152818989288, 0.7513927576601671, 0.9935933147632312)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   4/100, train loss: 7.49e-03, train acc: 7.659807102537891\n",
            "Epoch   4/100, val loss: 1.72e-02, val acc: 0.6165533786485405, val top5: 0.9831067572970812\n",
            "(0.032036310866018526, 0.48704735376044567, 0.9459610027855153)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   5/100, train loss: 6.73e-03, train acc: 7.717876350060003\n",
            "Epoch   5/100, val loss: 3.70e-02, val acc: 0.4775089964014394, val top5: 0.9170331867253099\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   6/100, train loss: 6.11e-03, train acc: 7.673852171207609\n",
            "Epoch   6/100, val loss: 9.78e-03, val acc: 0.7866853258696521, val top5: 0.9966013594562175\n",
            "(0.014638906169402567, 0.7937325905292479, 0.9889972144846797)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   7/100, train loss: 5.60e-03, train acc: 7.716854082403662\n",
            "Epoch   7/100, val loss: 1.54e-02, val acc: 0.6852259096361455, val top5: 0.9796081567373051\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   8/100, train loss: 5.06e-03, train acc: 7.726876750077781\n",
            "Epoch   8/100, val loss: 6.27e-02, val acc: 0.44412235105957615, val top5: 0.8827469012395042\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   9/100, train loss: 4.68e-03, train acc: 7.734565980710253\n",
            "Epoch   9/100, val loss: 2.47e-02, val acc: 0.6277489004398241, val top5: 0.9699120351859256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10/100, train loss: 2.61e-03, train acc: 7.834792657451442\n",
            "Epoch  10/100, val loss: 4.16e-03, val acc: 0.911235505797681, val top5: 0.9992003198720512\n",
            "(0.008965506002016387, 0.861699164345404, 0.9977715877437325)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11/100, train loss: 1.89e-03, train acc: 7.843281923641051\n",
            "Epoch  11/100, val loss: 4.92e-03, val acc: 0.8941423430627748, val top5: 0.9993002798880448\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12/100, train loss: 1.41e-03, train acc: 7.873394373083248\n",
            "Epoch  12/100, val loss: 6.65e-03, val acc: 0.8666533386645342, val top5: 0.9972011195521792\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13/100, train loss: 1.04e-03, train acc: 7.898595493133028\n",
            "Epoch  13/100, val loss: 4.54e-03, val acc: 0.9113354658136745, val top5: 0.999500199920032\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14/100, train loss: 8.49e-04, train acc: 7.8872172096537625\n",
            "Epoch  14/100, val loss: 3.99e-03, val acc: 0.919032387045182, val top5: 0.999000399840064\n",
            "(0.012285615655290051, 0.83008356545961, 0.9877437325905293)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15/100, train loss: 5.84e-04, train acc: 7.923152140095115\n",
            "Epoch  15/100, val loss: 3.99e-03, val acc: 0.9250299880047981, val top5: 0.999000399840064\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16/100, train loss: 4.78e-04, train acc: 7.928107915907374\n",
            "Epoch  16/100, val loss: 3.26e-02, val acc: 0.537485005997601, val top5: 0.9898040783686526\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17/100, train loss: 4.47e-04, train acc: 7.942664118405262\n",
            "Epoch  17/100, val loss: 5.78e-03, val acc: 0.9003398640543783, val top5: 0.9986005597760895\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18/100, train loss: 4.70e-04, train acc: 7.916974087737232\n",
            "Epoch  18/100, val loss: 6.02e-03, val acc: 0.8950419832067174, val top5: 0.9982007197121151\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19/100, train loss: 3.68e-04, train acc: 7.963620605360238\n",
            "Epoch  19/100, val loss: 1.06e-02, val acc: 0.8316673330667733, val top5: 0.9971011595361855\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:36<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20/100, train loss: 2.89e-04, train acc: 7.93701942308547\n",
            "Epoch  20/100, val loss: 1.07e-02, val acc: 0.8213714514194322, val top5: 0.9975009996001599\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21/100, train loss: 3.31e-04, train acc: 7.951842304102405\n",
            "Epoch  21/100, val loss: 6.10e-03, val acc: 0.8926429428228708, val top5: 0.9971011595361855\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22/100, train loss: 4.92e-04, train acc: 7.931796968754167\n",
            "Epoch  22/100, val loss: 1.42e-02, val acc: 0.7785885645741704, val top5: 0.9953018792483007\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23/100, train loss: 3.23e-04, train acc: 7.918796390950709\n",
            "Epoch  23/100, val loss: 7.07e-03, val acc: 0.8823470611755297, val top5: 0.9977009196321471\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24/100, train loss: 3.13e-04, train acc: 7.953575714476199\n",
            "Epoch  24/100, val loss: 1.07e-02, val acc: 0.8467612954818072, val top5: 0.9974010395841664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  25/100, train loss: 4.35e-04, train acc: 7.950353349037735\n",
            "Epoch  25/100, val loss: 7.36e-03, val acc: 0.8711515393842463, val top5: 0.9976009596161536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  26/100, train loss: 3.37e-04, train acc: 7.959575981154718\n",
            "Epoch  26/100, val loss: 6.08e-03, val acc: 0.9036385445821671, val top5: 0.998500599760096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  27/100, train loss: 2.56e-04, train acc: 7.955864705098004\n",
            "Epoch  27/100, val loss: 6.40e-03, val acc: 0.8993402638944422, val top5: 0.9977009196321471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [07:35<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  28/100, train loss: 2.29e-04, train acc: 7.970332014756211\n",
            "Epoch  28/100, val loss: 6.45e-03, val acc: 0.899640143942423, val top5: 0.9971011595361855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 311/704 [03:21<04:14,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-d1e969b06cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0md_label_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauxiliary_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0md_label_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mtop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "FloatTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "          nn.Conv2d(n_channels, 64, kernel_size=7, stride=1, padding='same', bias=False),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(),\n",
        "          ResidualStack(64, 64, 1, 6),\n",
        "          ResidualStack(64, 128, 2, 8),\n",
        "          ResidualStack(128, 256, 2, 12),\n",
        "          ResidualStack(256, 512, 2, 6),\n",
        "          nn.AdaptiveAvgPool2d(1),\n",
        "          Lambda(lambda x: x.squeeze()),\n",
        "      )\n",
        "    \n",
        "        # Output layers\n",
        "        self.adv_layer = nn.Sequential(nn.Linear(512, 1))\n",
        "        self.aux_layer = nn.Sequential(nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = self.main(img)\n",
        "        # out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)\n",
        "        label = self.aux_layer(out)\n",
        "\n",
        "        return validity, label   \n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim, img_size, channels):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
        "\n",
        "        self.init_size = img_size // 4  # Initial size before upsampling\n",
        "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        out = self.l1(noise)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = n_channels\n",
        "# Size of z latent vector \n",
        "nz = 50\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "num_classes = 9\n",
        "\n",
        "# A noise vector to be used for generating images at the end of each training epoch\n",
        "fixed_noise = torch.randn(ngf, nz, device=device)\n",
        "\n",
        "generator = Generator(num_classes, nz, 28, n_channels).to(device)\n",
        "# generator.apply(initialize_weight)\n",
        "discriminator = Discriminator(num_classes).to(device)\n",
        "discriminator.apply(initialize_weight)\n",
        "\n",
        "generator.load_state_dict(torch.load('Gan_G_copy50.pt'))\n",
        "# discriminator.load_state_dict(torch.load('Gan_D_copy50.pt'))\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.MultiStepLR(optimizer_G, milestones=[10, 50], gamma=0.1)\n",
        "\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler_D = torch.optim.lr_scheduler.MultiStepLR(optimizer_D, milestones=[10, 50], gamma=0.1)\n",
        "\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
        "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "best_model_wts = copy.deepcopy(discriminator.state_dict())\n",
        "best_loss = 100\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "\n",
        "    l_dataloader_iterator = iter(labeled_loader)\n",
        "\n",
        "    for inputs_u, _  in tqdm(unlabeled_loader):\n",
        "        try:\n",
        "            inputs_l, targets_l = next(l_dataloader_iterator)\n",
        "        except StopIteration:\n",
        "            l_dataloader_iterator = iter(labeled_loader)\n",
        "            inputs_l, targets_l = next(l_dataloader_iterator)\n",
        "        \n",
        "        inputs_l, targets_l = inputs_l.to(device), targets_l.to(device)\n",
        "        inputs_u = inputs_u.to(device)\n",
        "        batch_size = inputs_u.shape[0]\n",
        "\n",
        "        generator.train()\n",
        "        discriminator.train()\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
        "        \n",
        "        noise = torch.randn(batch_size, nz, device=device)\n",
        "        generated = generator(noise)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # TRAIN THE GENERATOR\n",
        "        \n",
        "        output, _ = discriminator(generated)\n",
        "        gen_loss = adversarial_loss(output, valid)\n",
        "        gen_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # TRAIN THE DISCRIMINATOR\n",
        "\n",
        "        # Loss for labeled images\n",
        "        _, label_aux = discriminator(inputs_l)\n",
        "        d_label_loss = auxiliary_loss(label_aux, targets_l.squeeze().long())\n",
        "        d_label_loss.backward()\n",
        "\n",
        "        top1 = torch.argmax(label_aux, dim=1).long()\n",
        "        ncorrect = torch.sum(top1 == targets_l)\n",
        "        epoch_loss += d_label_loss.item()\n",
        "        epoch_acc += ncorrect.item()\n",
        "\n",
        "        # Loss for unlabeled images\n",
        "        real_pred, _ = discriminator(inputs_u)\n",
        "        d_real_loss = adversarial_loss(real_pred, valid)\n",
        "        # d_real_loss.backward()\n",
        "\n",
        "        # Loss for fake images\n",
        "        fake_pred, _ = discriminator(generated.detach())\n",
        "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
        "        # d_fake_loss.backward()\n",
        "\n",
        "        d_loss_adv = (d_real_loss + d_fake_loss) / 2\n",
        "        d_loss_adv.backward()\n",
        "\n",
        "        optimizer_D.step()\n",
        "    \n",
        "    generated = (generator(fixed_noise)+1.0)/2.0\n",
        "    # vutils.save_image(generated.cpu().detach(), ('50generated_copy_%d.jpg' % epoch), normalize=True)\n",
        "    \n",
        "    val_loss, val_accuracy, val_top5 = test(discriminator, val_loader, 'val', criterion)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    epoch_loss /= len(unlabeled_loader.dataset)\n",
        "    epoch_acc /= len(unlabeled_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_acc)\n",
        "    \n",
        "    print(f\"Epoch {epoch: >3}/{NUM_EPOCHS}, train loss: {epoch_loss:.2e}, train acc: {epoch_acc}\")\n",
        "    print(f\"Epoch {epoch: >3}/{NUM_EPOCHS}, val loss: {val_loss:.2e}, val acc: {val_accuracy}, val top5: {val_top5}\")\n",
        "        \n",
        "    if val_loss < best_loss:\n",
        "        curr_patience = 0\n",
        "        best_loss = val_loss\n",
        "        best_classifier_wts = copy.deepcopy(discriminator.state_dict())\n",
        "        torch.save(discriminator.state_dict(), 'Gan_D_copy50.pt')\n",
        "        torch.save(generator.state_dict(), 'Gan_G_copy50.pt')\n",
        "        print(test(discriminator, test_loader, 'test', criterion))\n",
        "    else:\n",
        "        curr_patience += 1\n",
        "        if curr_patience == PATIENCE:\n",
        "          break\n",
        "    \n",
        "    lr_scheduler_D.step()\n",
        "    lr_scheduler_G.step()\n",
        "\n",
        "plt.plot(range(len(val_losses)), np.array(val_losses))\n",
        "plt.title('val loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(val_accuracies)), np.array(val_accuracies))\n",
        "plt.title('val acc')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(train_losses)), np.array(train_losses))\n",
        "plt.title('train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(train_accuracies)), np.array(train_accuracies))\n",
        "plt.title('train acc')\n",
        "plt.show()\n",
        "\n",
        "print('Best VAL test accuracy: ', np.max(np.array(val_accuracies)),\n",
        "      '% after ', np.argmax(np.array(val_accuracies)), ' training epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ewIbEg4zaJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48bb601-05d8-46d8-eedb-2c4bfa1fd29d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.012285615655290051, 0.83008356545961, 0.9877437325905293)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "model = Discriminator(num_classes).to(device)\n",
        "model.load_state_dict(best_classifier_wts)\n",
        "test(model, test_loader, 'test', criterion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(discriminator, test_loader, 'test', criterion)"
      ],
      "metadata": {
        "id": "E56Dv5L1AqOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}